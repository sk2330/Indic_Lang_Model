# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ewg7VgRpj-cue7uDwGbt8cf6KCbnNMob

#### Deep learning-based text normalisation model for Indic languages using Sarvam1-2B model
"""

!pip install indic-nlp-library
!pip install datasets

"""##### 1.Loading the Dataset from hugging face- Tamil and Malayalam"""

from datasets import load_dataset

ds = load_dataset("ai4bharat/IndicSentenceSummarization", "ta")
ds_mal=load_dataset("ai4bharat/IndicSentenceSummarization", "ml")

ds_mal

ds

"""#### 2. Loading/importing the Sarvam AI model"""

from datasets import concatenate_datasets
merged_train = concatenate_datasets([ds["train"], ds_mal["train"]]).shuffle(seed=42)
merged_validation = concatenate_datasets([ds["validation"], ds_mal["validation"]]).shuffle(seed=42)
merged_test = concatenate_datasets([ds["test"], ds_mal["test"]]).shuffle(seed=42)

# Create a merged dataset dictionary
merged_dataset = {
    "train": merged_train,
    "validation": merged_validation,
    "test": merged_test
}

print(merged_dataset)

"""#### 3.Hadling Entities"""

!pip install num2words tqdm regex
!pip install indic-numtowords

import re
from datasets import DatasetDict
from tqdm import tqdm
from indic_numtowords import num2words

SI_UNITS = {
    "kg": {"ta": "கிலோகிராம்", "ml": "കിലോഗ്രാം"},
    "km": {"ta": "கிலோமீட்டர்", "ml": "കിലോമീറ്റർ"},
    "cm": {"ta": "சென்டிமீட்டர்", "ml": "സെന്റിമീറ്റർ"},
    "m": {"ta": "மீட்டர்", "ml": "മീറ്റർ"},
    "g": {"ta": "கிராம்", "ml": "ഗ്രാം"},
    "l": {"ta": "லிட்டர்", "ml": "ലിറ്റർ"},
    "ml": {"ta": "மில்லிலிட்டர்", "ml": "മില്ലിലിറ്റർ"},
    "s": {"ta": "வினாடி", "ml": "സെക്കൻഡ്"},
    "K": {"ta": "கெல்வின்", "ml": "കൽവിൻ"},
    "m/s": {"ta": "மீட்டர் விநாடிக்கு", "ml": "മീറ്റർ പൂർവസെക്കൻഡ്"},
    "m²": {"ta": "சதுர மீட்டர்", "ml": "ചതുരശ്ര മീറ്റർ"},
    "m³": {"ta": "கூப்பியம் மீட்டர்", "ml": "ഘന മീറ്റർ"},
    "kg/m³": {"ta": "கிலோகிராம் மீட்டர் மூன்றுக்கு", "ml": "കിലോഗ്രാം പൂർവഘന മീറ്റർ"},
    "N·m": {"ta": "நியூட்டன் மீட்டர்", "ml": "ന്യൂട്ടൺ മീറ്റർ"},
    "J": {"ta": "ஜூல்", "ml": "ജൗൾ"},
    "W": {"ta": "வாட்", "ml": "വാട്ട്"},
    "Pa": {"ta": "பாஸ்கல்", "ml": "പാസ்கൽ"},
    "V": {"ta": "வோல்ட்", "ml": "വോൾട്ട്"},
    "Ω": {"ta": "ஓம்", "ml": "ഓം"},
    "Hz": {"ta": "ஹெர்ட்ஸ்", "ml": "ഹെർട്സ്"},
    "C": {"ta": "கூலோம்ப்", "ml": "കൂളോമ്പ്"},
    "km/h": {"ta": "கிலோமீட்டர் மணிக்கு", "ml": "കിലോമീറ്റർ പൂർവമണിക്കൂർ"},
}

CURRENCY_UNITS = {
    "₹": {"ta": "ரூபாய்", "ml": "രൂപ"},
    "INR": {"ta": "ரூபாய்", "ml": "രൂപ"},
    "Rs.": {"ta": "ரூபாய்", "ml": "രൂപ"},
    "$": {"ta": "டாலர்", "ml": "ഡോളർ"},
    "€": {"ta": "யூரோ", "ml": "യൂറോ"},
    "£": {"ta": "பவுண்ட்", "ml": "പൗണ്ട്"},
    "¥": {"ta": "யென்", "ml": "യെൻ"},
    "₽": {"ta": "ரூபிள்", "ml": "റൂബിൾ"},
    "₩": {"ta": "வான்", "ml": "വോൺ"},
    "₺": {"ta": "லீரா", "ml": "ലിറ"},
    "฿": {"ta": "பாத்", "ml": "ബാത്ത്"},
    "₦": {"ta": "நைரா", "ml": "നൈറ"}
}

def convert_to_words(text, lang):
    def replace_numbers(match):
        return num2words(match.group(), lang=lang)


    def replace_dates(match):
        day, month, year = match.groups()
        return f"{num2words(day, lang=lang)} {num2words(month, lang=lang)} {num2words(year, lang=lang)}"

    def replace_currency(match):
        amount, currency = match.groups()
        currency_translation = CURRENCY_UNITS.get(currency.strip(), {}).get(lang, currency)
        return f"{num2words(amount, lang=lang)} {currency_translation}"

    words = text.split()
    for i, word in enumerate(words):
        if re.match(r"^\d+(\.\d+)?$", word):
            next_word = words[i + 1] if i + 1 < len(words) else ""
            if next_word in SI_UNITS:
                unit_translation = SI_UNITS[next_word].get(lang, next_word)
                words[i] = num2words(word, lang=lang)
                words[i + 1] = unit_translation
    text = " ".join(words)
    text = re.sub(r"(\d{1,2})[-/](\d{1,2})[-/](\d{2,4})", replace_dates, text)
    text = re.sub(r"(\d+(?:\.\d+)?)\s*(₹|INR|Rs\.?|\$|€|£|¥|₽|₩|₺|฿|₦)", replace_currency, text)

    return text

"""#### 4.Testing the code with sample data

"""

sample_texts = [
    "நீங்கள் 25 l பரிசு வென்றுள்ளீர்கள்.",
    "ഒരു 12 cm നീളമുള്ള ലോഹ കമ്പി ആവശ്യമാണ്.",
    "குறைந்தபட்சம் 300 $ செலவு செய்ய வேண்டும்.",
    "22-01-2024 அன்று பரீட்சை நடக்கிறது.",
]

for text in sample_texts:
    print("Original:", text)
    print("Processed Tamil:", convert_to_words(text, "ta"))
    print("Processed Malayalam:", convert_to_words(text, "ml"))
    print("------")

"""#### 5.Pushing dataset to HF"""

import os
HUGGING_FACK_TOKEN = os.environ.get("HUGGING_FACE_TOKEN")

from huggingface_hub import login

login()

from datasets import load_dataset

hf_dataset = DatasetDict({
    "train": merged_train,
    "validation": merged_validation,
    "test": merged_test
})
print(hf_dataset)

from huggingface_hub import HfApi

repo_name = "ml_ta_text_normalization"
username = "Saikrishna2403"

api = HfApi()
api.create_repo(repo_id=f"{username}/{repo_name}", repo_type="dataset")

###pushing dataset to HF
hf_dataset.push_to_hub(f"{username}/{repo_name}")

# Loading the dataset from HF
df = load_dataset("Saikrishna2403/ml_ta_text_normalization")

df

df=df.remove_columns(['id','url'])
df

print(df['train'][3])

!pip install langdetect

from langdetect import detect

def preprocess_function(examples):
    processed_inputs = []
    processed_targets = []

    for input_text, target_text in zip(examples["input"], examples["target"]):
        try:
            lang = detect(input_text)

            if lang in ["ta", "ml"]:
                processed_inputs.append(convert_to_words(input_text, lang=lang))
                processed_targets.append(convert_to_words(target_text, lang=lang))
            else:
                processed_inputs.append(input_text)
                processed_targets.append(target_text)
        except:
            processed_inputs.append(input_text)
            processed_targets.append(target_text)

    return {"input": processed_inputs, "target": processed_targets}

"""##### 6.Tokenize the dataset"""

final_ds = df.map(preprocess_function, batched=True)

print(type(df))
print(type(final_ds))

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "sarvamai/sarvam-1"  # Sarvam AI model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

import pandas as pd
from datasets import Dataset

if isinstance(df, pd.DataFrame):
    print("df is a Pandas DataFrame")
elif isinstance(df, Dataset):
    print("df is a Hugging Face Dataset")
else:
    print("df is neither a Pandas DataFrame nor a Hugging Face Dataset")

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("sarvamai/sarvam-1")

def tokenize_function(examples):
    return tokenizer(examples["input"], padding="max_length", truncation=True)

tokenized_ds = final_ds.map(tokenize_function, batched=True)

from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("sarvamai/sarvam-1")

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-3,
    weight_decay=0.01,
    num_train_epochs=3,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds["train"],
    eval_dataset=tokenized_ds["validation"],
    tokenizer=tokenizer,
)

trainer.train()

"""#### Evaluatig the metrics"""

from datasets import load_metric
import numpy as np

bleu_metric = load_metric("bleu")
chrf_metric = load_metric("chrf")
wer_metric = load_metric("wer")
cer_metric = load_metric("cer")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    #Covertig tokens back to text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    ###Removig extra spaces etc
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [label.strip() for label in decoded_labels]

    bleu_score = bleu_metric.compute(predictions=[decoded_preds], references=[[decoded_labels]])["bleu"]
    chrf_score = chrf_metric.compute(predictions=[decoded_preds], references=[[decoded_labels]])["score"]
    wer_score = wer_metric.compute(predictions=[decoded_preds], references=[[decoded_labels]])
    cer_score = cer_metric.compute(predictions=[decoded_preds], references=[[decoded_labels]])

    return {
        "bleu": bleu_score,
        "chrf": chrf_score,
        "wer": wer_score,
        "cer": cer_score,
    }

    # Run predictions on the validation set
predictions = trainer.predict(tokenized_ds["validation"])

# Compute evaluation metrics
metrics = compute_metrics(predictions)

# Print the evaluation metrics
print("\nEvaluation Metrics:")
for key, value in metrics.items():
    print(f"{key.upper()}: {value:.4f}")

